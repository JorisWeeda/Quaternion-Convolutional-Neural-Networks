{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15483,"status":"ok","timestamp":1681579189979,"user":{"displayName":"joris weeda","userId":"16756172332098140791"},"user_tz":-120},"id":"xq_wUFIhKkPK","outputId":"c30ac206-04b0-4de5-96fc-3c3fb80dd446"},"outputs":[{"name":"stdout","output_type":"stream","text":["Python 3.9.16\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: cifar10 in /usr/local/lib/python3.9/dist-packages (1.0.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from cifar10) (1.22.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from cifar10) (2.27.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from cifar10) (4.65.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->cifar10) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->cifar10) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->cifar10) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->cifar10) (3.4)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: imageio in /usr/local/lib/python3.9/dist-packages (2.25.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (1.22.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (1.10.1)\n","Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.9/dist-packages (from imageio) (8.4.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/Orkis-Research/Pytorch-Quaternion-Neural-Networks\n","  Cloning https://github.com/Orkis-Research/Pytorch-Quaternion-Neural-Networks to /tmp/pip-req-build-tbs74fkr\n","  Running command git clone --filter=blob:none --quiet https://github.com/Orkis-Research/Pytorch-Quaternion-Neural-Networks /tmp/pip-req-build-tbs74fkr\n","  Resolved https://github.com/Orkis-Research/Pytorch-Quaternion-Neural-Networks to commit 28caa7cde240e354fd7b87280450fd233cd494c3\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["# run this cell to download the right packages (only needed once)\n","!python --version\n","\n","!pip install cifar10\n","!pip install imageio numpy scipy    \n","!pip install git+https://github.com/Orkis-Research/Pytorch-Quaternion-Neural-Networks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9bXMRTLTxIJ9"},"outputs":[],"source":["import time\n","import torch\n","\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from pathlib import Path\n","from torch.utils.data import DataLoader\n","from torchsummary import summary\n","from torchvision import datasets, transforms\n","\n","from core_qnn.quaternion_layers import QuaternionConv, QuaternionLinear\n","from core_qnn.quaternion_ops import check_input, q_normalize\n","\n","device = torch.device('cuda' if torch.cuda.is_available else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1864,"status":"ok","timestamp":1681579194177,"user":{"displayName":"joris weeda","userId":"16756172332098140791"},"user_tz":-120},"id":"vfTDOOC8ruuV","outputId":"8e079beb-674e-4be1-f0b4-6b37e4f7d21e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","CPU times: user 1.41 s, sys: 273 ms, total: 1.68 s\n","Wall time: 1.68 s\n"]}],"source":["%%time\n","\n","# import and download the CIFAR10 dataset\n","transform_train = transforms.Compose([transforms.ToTensor(), transforms.Normalize((.5,.5,.5),(.5,.5,.5))])\n","transform_test = transforms.Compose([transforms.ToTensor(), transforms.Normalize((.5,.5,.5),(.5,.5,.5))])\n","\n","train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n","test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n","\n","classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6iEEcR2ofk6L"},"outputs":[],"source":["class InvalidKernelShape(RuntimeError):\n","  \"\"\"Base class to generate custom exception if generating kernel failed.\"\"\"\n","\n","  def __init__(self, error_message):\n","    \"\"\" Construct custom error with custom error message.\n","    :param error_message: The custom error message.\n","    \"\"\"\n","    super().__init__(error_message)\n","\n","class InvalidInput(RuntimeError):\n","  \"\"\"Base class to generate custom exception if input is invalid.\"\"\"\n","\n","  def __init__(self, error_message):\n","    \"\"\" Construct custom error with custom error message.\n","    :param error_message: The custom error message.\n","    \"\"\"\n","    super().__init__(error_message)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bq56qgrC3OLQ"},"outputs":[],"source":["class QuaternionConvolution(nn.Module):\n","  \"\"\"Reproduction class of the quaternion convolution layer.\"\"\"\n","\n","  ALLOWED_DIMENSIONS = (2, 3)\n","\n","  def __init__(self, in_channels, out_channels, kernel_size, stride, dimension=2, padding=0, dilation=1, groups=1, bias=True):\n","    \"\"\"Create the quaterion convolution layer.\"\"\"\n","    super(QuaternionConvolution, self).__init__()\n","\n","    self.in_channels = np.floor_divide(in_channels, 4)\n","    self.out_channels = np.floor_divide(out_channels, 4)\n","\n","    self.groups = groups\n","    self.stride = stride\n","    self.padding = padding\n","    self.dilation = dilation\n","\n","    self.kernel_size = self.get_kernel_shape(kernel_size, dimension)\n","    self.weight_shape = self.get_weight_shape(self.in_channels, self.out_channels, self.kernel_size)\n","\n","    self._weights = self.weight_tensors(self.weight_shape, kernel_size)\n","    self.r_weight, self.k_weight, self.i_weight, self.j_weight = self._weights\n","    \n","    if bias:\n","      self.bias = nn.Parameter(torch.Tensor(out_channels))\n","      nn.init.constant_(self.bias, 0)\n","\n","  def forward(self, x):\n","    \"\"\"Apply forward pass of input through quaternion convolution layer.\"\"\"\n","    cat_kernels_4_r = torch.cat([self.r_weight, -self.i_weight, -self.j_weight, -self.k_weight], dim=1)\n","    cat_kernels_4_i = torch.cat([self.i_weight,  self.r_weight, -self.k_weight, self.j_weight], dim=1)\n","    cat_kernels_4_j = torch.cat([self.j_weight,  self.k_weight, self.r_weight, -self.i_weight], dim=1)\n","    cat_kernels_4_k = torch.cat([self.k_weight,  -self.j_weight, self.i_weight, self.r_weight], dim=1)\n","\n","    cat_kernels_4_quaternion   = torch.cat([cat_kernels_4_r, cat_kernels_4_i, cat_kernels_4_j, cat_kernels_4_k], dim=0)\n","\n","    if x.dim() == 3:\n","        convfunc = F.conv1d\n","    elif x.dim() == 4:\n","        convfunc = F.conv2d\n","    elif x.dim() == 5:\n","        convfunc = F.conv3d\n","    else:\n","        raise InvalidInput(\"Given input channels do not match allowed dimensions\")\n","\n","    return convfunc(x, cat_kernels_4_quaternion, self.bias, self.stride, self.padding, self.dilation, self.groups)\n","\n","  @staticmethod\n","  def weight_tensors(weight_shape, kernel_size):\n","    \"\"\"Create and initialise the weight tensors according to quaternion rules.\"\"\"\n","    modulus = nn.Parameter(torch.Tensor(*weight_shape))\n","    modulus = nn.init.xavier_uniform_(modulus, gain=1.0)\n","\n","    i_weight = 2.0 * torch.rand(*weight_shape) - 1.0\n","    j_weight = 2.0 * torch.rand(*weight_shape) - 1.0\n","    k_weight = 2.0 * torch.rand(*weight_shape) - 1.0\n","\n","    sum_imaginary_parts = i_weight.abs() + j_weight.abs() + k_weight.abs()\n","\n","    i_weight = torch.div(i_weight, sum_imaginary_parts)\n","    j_weight = torch.div(j_weight, sum_imaginary_parts)\n","    k_weight = torch.div(k_weight, sum_imaginary_parts)\n","\n","    phase = torch.rand(*weight_shape) * (2 * torch.tensor([np.pi])) - torch.tensor([np.pi])\n","\n","    r_weight = modulus * np.cos(phase)\n","    i_weight = modulus * i_weight * np.sin(phase)\n","    j_weight = modulus * j_weight * np.sin(phase)\n","    k_weight = modulus * k_weight * np.sin(phase)\n","\n","    return nn.Parameter(r_weight), nn.Parameter(i_weight), nn.Parameter(j_weight), nn.Parameter(k_weight)\n","\n","  @staticmethod\n","  def get_weight_shape(in_channels, out_channels, kernel_size):\n","    \"\"\"Construct weight shape based on the input/output channels and kernel size.\"\"\"\n","    return (out_channels, in_channels) + kernel_size\n","\n","  @staticmethod\n","  def get_kernel_shape(kernel_size, dimension):\n","    \"\"\"Construct the kernel shape based on the given dimension and kernel size.\"\"\"\n","    if dimension not in QuaternionConvolution.ALLOWED_DIMENSIONS:\n","      raise InvalidKernelShape('Given dimensions are not allowed.')\n","    \n","    if isinstance(kernel_size, int):\n","      return (kernel_size, ) * dimension\n","\n","    if isinstance(kernel_size, tuple):\n","      if len(kernel_size) != dimension:\n","        raise InvalidKernelShape('Given kernel shape does not match dimension.')\n","\n","      return kernel_size\n","\n","    raise InvalidKernelShape('No valid type of kernel size to construct kernel.')\n","\n","  def __repr__(self):\n","      return self.__class__.__name__ + '(' \\\n","          + 'in_channels='      + str(self.in_channels) \\\n","          + ', out_channels='   + str(self.out_channels) \\\n","          + ', kernel_size='    + str(self.kernel_size) \\\n","          + ', stride='         + str(self.stride) + ')'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2170,"status":"ok","timestamp":1681579196339,"user":{"displayName":"joris weeda","userId":"16756172332098140791"},"user_tz":-120},"id":"bn_GMalk2BoY","outputId":"a2f9d742-31ae-4a05-d735-942fd452ba1e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of trainable parameters:  2032650\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","QuaternionConvolution-1           [32, 64, 30, 30]              64\n","QuaternionConvolution-2          [32, 128, 28, 28]             128\n","         MaxPool2d-3          [32, 128, 14, 14]               0\n","           Dropout-4          [32, 128, 14, 14]               0\n","QuaternionConvolution-5          [32, 256, 12, 12]             256\n","QuaternionConvolution-6          [32, 512, 10, 10]             512\n","         MaxPool2d-7            [32, 512, 5, 5]               0\n","           Dropout-8            [32, 512, 5, 5]               0\n","  QuaternionLinear-9                  [32, 512]             512\n","          Dropout-10                  [32, 512]               0\n","           Linear-11                   [32, 10]           5,130\n","          Softmax-12                   [32, 10]               0\n","================================================================\n","Total params: 6,602\n","Trainable params: 5,130\n","Non-trainable params: 1,472\n","----------------------------------------------------------------\n","Input size (MB): 0.50\n","Forward/backward pass size (MB): 78.82\n","Params size (MB): 0.03\n","Estimated Total Size (MB): 79.34\n","----------------------------------------------------------------\n","CPU times: user 2.24 s, sys: 137 ms, total: 2.38 s\n","Wall time: 2.46 s\n"]}],"source":["%%time\n","\n","class CustomQCNN(nn.Module):\n","  \"\"\"Reproduction QCNN to validate quaternion convolution layer.\"\"\"\n","\n","  def __init__(self, in_channels, hidden_channels, out_features, kernel_size):\n","    super(CustomQCNN, self).__init__()\n","\n","    self.conv_1 = QuaternionConvolution(in_channels, hidden_channels[0], kernel_size, 1)\n","    self.conv_2 = QuaternionConvolution(hidden_channels[0], hidden_channels[1], kernel_size, 1)\n","\n","    self.pool_1 = nn.MaxPool2d(2, 2)\n","    self.dropout_1 = nn.Dropout(0.25)\n","\n","    self.conv_3 = QuaternionConvolution(hidden_channels[1], hidden_channels[2], kernel_size, 1)\n","    self.conv_4 = QuaternionConvolution(hidden_channels[2], hidden_channels[3], kernel_size, 1)\n","\n","    self.pool_2 = nn.MaxPool2d(2, 2)\n","    self.dropout_2 = nn.Dropout(0.25)\n","\n","    self.fc_1 = QuaternionLinear(12800, 512)\n","    self.fc_2 = nn.Linear(512, out_features)\n","\n","    self.dropout_3 = nn.Dropout(0.5)\n","    self.sm = nn.Softmax(dim=1)\n","\n","  def forward(self, x):\n","    x = F.relu(self.conv_1(x))\n","    x = F.relu(self.conv_2(x))\n","    x = self.pool_1(x)\n","    x = self.dropout_1(x)\n","\n","    x = F.relu(self.conv_3(x))\n","    x = F.relu(self.conv_4(x))\n","    x = self.pool_2(x)\n","    x = self.dropout_2(x)\n","\n","    x = torch.flatten(x, start_dim=1) \n","\n","    x = F.relu(self.fc_1(x))\n","    x = self.dropout_3(x)\n","    x = self.fc_2(x)\n","    x = self.sm(x)\n","\n","    return x\n","\n","# Model parameters\n","in_channels = 4\n","hidden_channels = [64, 128, 256, 512]\n","out_features = 10\n","kernel_size = (3, 3)\n","\n","batch_size = 32\n","\n","custom_qcnn = CustomQCNN(in_channels, hidden_channels, out_features, kernel_size)\n","custom_qcnn = custom_qcnn.cuda()\n","\n","print(\"Number of trainable parameters: \", sum(p.numel() for p in custom_qcnn.parameters() if p.requires_grad))\n","summary(custom_qcnn, input_size=(in_channels, 32, 32), batch_size=batch_size, device=device.type)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1681579196340,"user":{"displayName":"joris weeda","userId":"16756172332098140791"},"user_tz":-120},"id":"4waYaaGMKp2n","outputId":"88e0fba2-0829-4b1c-c68e-d1b3e410e8ae"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of trainable parameters:  640\n","tensor([[[ 4.9678e-02,  1.7548e-02, -1.5976e-01],\n","         [-1.5517e-02, -3.2999e-02, -3.3856e-05],\n","         [ 3.9830e-02, -1.6522e-02, -8.2282e-03]]], grad_fn=<SelectBackward0>)\n","CPU times: user 7.28 ms, sys: 0 ns, total: 7.28 ms\n","Wall time: 7.43 ms\n"]}],"source":["%%time\n","paper_qcnn_layer = QuaternionConv(4, 64, kernel_size, stride=1)\n","\n","print(\"Number of trainable parameters: \", sum(p.numel() for p in paper_qcnn_layer.parameters() if p.requires_grad))\n","print(paper_qcnn_layer.i_weight[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1681579196342,"user":{"displayName":"joris weeda","userId":"16756172332098140791"},"user_tz":-120},"id":"nvSKH5k7iYCK","outputId":"bde6e926-01d9-449c-855b-b4a1ed508967"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of trainable parameters:  640\n","tensor([[[ 8.9675e-03, -1.2020e-02,  5.8783e-02],\n","         [-2.6882e-05,  5.7910e-02, -4.7084e-02],\n","         [ 1.3167e-02,  7.9821e-02,  7.1172e-02]]], grad_fn=<SelectBackward0>)\n","CPU times: user 4.68 ms, sys: 0 ns, total: 4.68 ms\n","Wall time: 5.78 ms\n"]}],"source":["%%time\n","custom_qcnn_layer = QuaternionConvolution(4, 64, kernel_size, 1)\n","\n","print(\"Number of trainable parameters: \", sum(p.numel() for p in custom_qcnn_layer.parameters() if p.requires_grad))\n","print(custom_qcnn_layer.i_weight[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"OyAWqLtFrLGY","outputId":"eea48d68-cc7c-4e12-93f3-a2e8b852516e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Start training seed 1\n","Epoch [11/80], Last loss: 2.0787\n","Epoch [21/80], Last loss: 1.7392\n","Epoch [31/80], Last loss: 1.6275\n","Epoch [41/80], Last loss: 1.6913\n","Epoch [51/80], Last loss: 1.5220\n","Epoch [61/80], Last loss: 1.6078\n","Epoch [71/80], Last loss: 1.6412\n","Finished training seed 1, accuracy of the network: 78.45%, elapsed time: 1957 sec\n","Start training seed 2\n","Epoch [11/80], Last loss: 1.9188\n","Epoch [21/80], Last loss: 1.6543\n","Epoch [31/80], Last loss: 1.6585\n","Epoch [41/80], Last loss: 1.5151\n","Epoch [51/80], Last loss: 1.6406\n","Epoch [61/80], Last loss: 1.7523\n","Epoch [71/80], Last loss: 1.4807\n","Finished training seed 2, accuracy of the network: 77.72%, elapsed time: 1963 sec\n","Start training seed 3\n","Epoch [11/80], Last loss: 1.9496\n","Epoch [21/80], Last loss: 1.7193\n","Epoch [31/80], Last loss: 1.6582\n","Epoch [41/80], Last loss: 1.6938\n","Epoch [51/80], Last loss: 1.5250\n","Epoch [61/80], Last loss: 1.7079\n","Epoch [71/80], Last loss: 1.5793\n","Finished training seed 3, accuracy of the network: 78.14%, elapsed time: 1954 sec\n","Average accuracy over 3, 80 epochs each results in: 78.10333333333334\n","CPU times: user 1h 5min 2s, sys: 3min 18s, total: 1h 8min 21s\n","Wall time: 1h 37min 55s\n"]}],"source":["%%time\n","num_epochs = 80\n","amount_of_trainings = 3\n","\n","learning_rate = 0.0001\n","learning_rate_decay = 1e-6\n","\n","batch_size = 32\n","\n","custom_qcnn_accs = []\n","trainings_seed_excution_time = []\n","\n","for training_seed in range(amount_of_trainings):\n","  print(f'Start training seed {training_seed + 1}')\n","  start_time_training_seed = time.time()\n","\n","  train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n","  test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2)\n","\n","  custom_qcnn = CustomQCNN(in_channels, hidden_channels, out_features, kernel_size)\n","  custom_qcnn = custom_qcnn.cuda()\n","\n","  optimizer = torch.optim.RMSprop(custom_qcnn.parameters(),lr=learning_rate, weight_decay=learning_rate_decay)\n","  criterion = nn.CrossEntropyLoss()\n","\n","  for epoch in range(1, num_epochs):\n","    \n","    custom_qcnn.train()\n","\n","    for index, (x_batch, y_batch) in enumerate(train_loader):\n","      zeros_channel = torch.zeros((x_batch.shape[0], 1, x_batch.shape[2], x_batch.shape[3]))\n","      x_batch = torch.cat([x_batch, zeros_channel], dim=1)\n","\n","      # Check if the input size is correct\n","      check_input(x_batch)\n","\n","      x_batch = x_batch.cuda()\n","      y_batch = y_batch.cuda()\n","      \n","      # Perform forward pass\n","      y_pred = custom_qcnn(x_batch)\n","\n","      # Compute the loss\n","      loss = criterion(y_pred, y_batch)\n","\n","      # Backpropagation\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","\n","    if (epoch / 10).is_integer():\n","      print (f'Epoch [{epoch + 1}/{num_epochs}], Last loss: {loss.item():.4f}')\n","\n","  with torch.no_grad():\n","      n_correct = 0\n","      n_samples = 0\n","\n","      custom_qcnn.eval()\n","\n","      for index, (x_batch, y_batch) in enumerate(test_loader):\n","        zeros_channel = torch.zeros((x_batch.shape[0], 1, x_batch.shape[2], x_batch.shape[3]))\n","        x_batch = torch.cat([x_batch, zeros_channel], dim=1)\n","\n","        x_batch = x_batch.cuda()\n","        y_batch = y_batch.cuda()\n","\n","        # Check if the input size is correct\n","        check_input(x_batch)\n","\n","        # Perform forward pass\n","        y_pred = custom_qcnn(x_batch)\n","\n","        _, predicted = torch.max(y_pred,1)\n","        n_samples += y_batch.size(0)\n","        n_correct += (predicted == y_batch).sum().item()\n","\n","      acc = 100 * n_correct / n_samples\n","      custom_qcnn_accs.append(acc)\n","  \n","  elapsed_training_time = int(time.time() - start_time_training_seed)\n","  trainings_seed_excution_time.append(start_time_training_seed)\n","\n","  print(f'Finished training seed {training_seed + 1}, accuracy of the network: {acc}%, elapsed time: {elapsed_training_time} sec')\n","\n","print(f'Average accuracy over {amount_of_trainings}, {num_epochs} epochs each results in: {sum(custom_qcnn_accs) / amount_of_trainings}')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}